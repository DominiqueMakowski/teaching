---
title: "Bayesian Statistics"
subtitle: "**9. Linear Models (Posterior)**"
author: "<sub>Dominique Makowski</sub><br><sub><sup>*D.Makowski@sussex.ac.uk*</sup></sub>"
# institute: "University of Sussex"
title-slide-attributes:
  data-background-image: "https://github.com/RealityBending/RealityBending.github.io/blob/main/assets/media/sussex.png?raw=true"
  data-background-opacity: "0.2"
  data-background-color: "black"
  # data-background-size: contain
format:
  revealjs:
    logo: "https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/University_of_Sussex_Logo.svg/480px-University_of_Sussex_Logo.svg.png"
    incremental: true
    chalkboard: true
    scrollable: true
    slide-number: "c/t"
    highlight-style: "github-dark"
    code-line-numbers: false
    fontsize: "170%"
    # theme: blood
    # title-slide-attributes:
    #   data-background-color: "#1A3F82"
editor_options: 
  chunk_output_type: console
execute:
  cache: true
fig-dpi: 300
params:
  show_answers: true
---

```{r}
#| include: false

library(tidyverse)
library(patchwork)
library(easystats)

set.seed(333)
```

## Recap {background-color="#003B49"}

1. Formula specification

::: {.fragment}

```{r}
#| echo: true

library(brms)

f <- brms::brmsformula(qsec ~ 0 + Intercept + mpg)
```

:::

2. Prior specification

- Intercept
  - e.g., $\mu$ = `mean(mtcars$qsec)` and $\sigma$ = `10 * sd(mtcars$qsec)`
- Slope
  - e.g., $\mu$ = 0 and $\sigma$ = 5

::: {.fragment}

```{r}
#| echo: true

priors <- c(
  brms::set_prior("normal(17.85, 17.9)", class = "b", coef = "Intercept"),
  brms::set_prior("normal(0, 5)", class = "b", coef = "mpg")
) |> 
  brms::validate_prior(f, data=mtcars)

priors
```

:::

3. Prior Predictive Checks

- Predicted response density and relationships based on the priors only

## Fitting the model

- Pass the formula, the data, and the priors (optional) to `brms::brm()` ("Bayesian Regression Model")

::: {.fragment}

```{r}
#| echo: true
#| eval: false

model <- brms::brm(f, 
                   data = mtcars, 
                   prior = priors)
```

:::

- Options pertaining to the **sampling algorithm** can be specified^[See the function documentation for the massive list of options]

::: {.fragment}

```{r}
#| echo: true

model <- brms::brm(f, 
                   data = mtcars, 
                   prior = priors,
                   chains = 4, # Number of independent sampling processes 
                   iter = 1000,  # Number of draws per chain
                   refresh = 0)  # Print progress
```

:::

## MCMC - NUTS

::: {.columns}

:::: {.column width=50%}

- By default, `brms` uses MCMC
  - In particular, the **No-U-Turn Sampler (NUTS)** algorithm, which is a self-adjusting variant of **Hamiltonian Monte Carlo (HMC)**
  - The key idea behind HMC is that it treats sampling as a physics problem, where we imagine the posterior distribution space as a hilly landscape and use momentum to explore it efficiently
  - Instead of setting a fixed number of steps, NUTS keeps "rolling the ball" until it detects a U-turn (where the trajectory starts going back toward its starting point)
  
:::: 

:::: {.column}

![](img/hmc.gif)

::::

:::

## MCMC vs. Variational Inference (VI)

::: {.columns}

:::: {.column width=70% style="font-size: 70%;"}

- Stan (the language used by `brms` in the background) also implements **variational inference** algorithms that sample from an approximation of the posterior, which are faster but less accurate
  - You can use them by specifying: `brm(..., algorithm = "meanfield")` (or`"fullrank"` or `"pathfinder"`)
  - Unlike MCMC, which samples from the true posterior, Variational Inference (VI) approximates it using simpler distribution (e.g., Gaussian) and minimizes the difference between the two (e.g., optimizing the Kullback-Leibler KL divergence)
- **Pros:**
  - Can be useful for "quick" checks and adjustments but should not necessarily be used for final results
  - If the uncertainty estimation is not the main goal but speed is
- **Cons:**
  - Less accurate
  - VI posteriors may be overconfident (narrower CI)

:::: 

:::: {.column width=30%}

![](img/KullbackLeibler.png)

::::

:::

## VI in the Brain

::: {.columns}

:::: {.column width=70% style="font-size: 70%;"}


- The brain is thought to be a Bayesian inference machine (Friston et al.), constantly predicting sensory input and updating beliefs based on errors
- **Prediction errors** (mismatch between expectation and reality) drive learning, perception, and action
- Computing exact posteriors would be too computationally expensive for the brain
- Instead, it might approximates the posterior in a way similar to VI:
  - The brain represents beliefs with simpler distributions (e.g., Gaussian-like approximations).
  - Instead of exploring all possibilities, it minimizes a function ("Free Energy") to find the best fit
- The brain minimizes Free Energy to improve its model could be mathematically similar to VI minimizing the Kullback-Leibler (KL) divergence between an approximate posterior and the true posterior
- Cognitive and neural processing could be optimized for **efficiency** rather than exactness, much like how VI trades accuracy for speed

:::: 

:::: {.column width=30%}

![](img/PredictiveProcessing.jpg)

::::

:::

## MCMC parameters

- For this module we will focus on MCMC, which is slowest but the "true" Bayesian solution
- MCMC can be tuned
  - Number of **chains**, number of **iterations**
  - **Warm-up**: the number of initial samples that are discarded (burn-in phase)
  - **Thinning**: the number of samples that are kept (every n-th sample)
  - **Adaptation**: the algorithm can adapt its parameters during the burn-in phase
  - ...
- In most cases, the default values are fine
- It's only when there are problems that you should start considering tweaking these parameters


## Sampling Diagnostics - Trace Plots 

::: {.columns}

:::: {.column width=70%}


- **HAIRY CATERPILLARS**
- The MCMC algorithm is supposed to draw **independent** samples from the posterior
- There should be no autocorrelation between successive samples (i.e., no patterns in the trace plot)
  - They should look like [**hairy caterpillars**]{.fragment}
- We typically draw multiple independent **chains** (useful for parallel computing, e.g., one chain per core)

::::

:::: {.column width=30%}

![](img/colin.webp)

::::

:::

::: {.fragment}

```{r}
#| echo: true
#| fig-width: 12

plot(model)
```

:::

- The plots shows 1) the **posterior** distribution of the parameters and 2) the **trace** plot of the MCMC algorithm
- One trace per chain: there should not be any pattern 
  
## Sampling Diagnostics - Effective Sample Size (ESS)

```{r}
#| echo: true

model
```


- The **effective sample size** (ESS) is an estimation of the number of independent samples that we have
  - ESS ("Bulk" ESS) is a measure of how well the **centre** of the posterior distribution is described
  - Can be see as an index of the accuracy of the indices of centrality
  - "Tail" ESS is a measure of how well the **tails** of the distribution are described
  - Can be see as an index of the accuracy of the indices based on range
- ESS should be at least 1000 (see `effectsize::interpret_ess()`)

## Sampling Diagnostics - $\hat{R}$

- The $\hat{R}$ ("R-hat") is a measure of **convergence**
- It compares the **variance** within chains to the **variance** between chains
- If the chains have not converged to a common distribution, the $\hat{R}$ statistic will be greater than one
- Should be lower than 1.01 (see `effectsize::interpret_rhat()`)

## Posterior Description


```{r}
#| echo: true

parameters::parameters(model)
```

- Indices of centrality, uncertainty, existence, significance
- Customize the parameters shown

::: {.fragment}

```{r}
#| echo: true

parameters::parameters(model,
                       centrality="mean", 
                       dispersion=TRUE,
                       ci=0.89)
```

:::


## Effect Significance

```{r}
#| echo: false

parameters::parameters(model)
```


- Easiest is to use *pd* or decision threshold based on CI overlapping 0 (similar to Frequentist NHST)
- **ROPE?** But what ROPE bounds to use?
  - Not straightforward to define
  - Easier to use when data is standardized (and parameters are expressed in terms of SD)
  - Or when you have clear hypotheses on the effect size
- **BF?** 
  - Complicated to compute for model parameters
  
  
## Model Performance - R2

- Once that sampling quality has been assessed, and that the posteriors have been described, we can assess the model's performance
- How well does the model fit/predict the data?
- R-squared, the percentage of variance explained by the model (for linear models), can be computed

::: {.fragment}

```{r}
#| echo: true

performance::r2(model)
```

:::

- With Bayesian statistics - where every parameter is **probabilistic**, we also get a posterior distribution of the R-squared, and can thus compute **credible intervals**
- Note that R-squared is straightforward to interpret for **linear** model but gets tricky for GLMs (because there is no "variance" of the outcome in the same sense). We cannot say "the model explains X% of the variance" baed on the R2 for GLMs.

## Model Performance - Relative Indices

- Other **"relative" indices of fit** exist to compare models between them (we will see that later)

::: {.fragment}

```{r}
#| echo: true

performance::performance(model)
```

:::

## Posterior Predictive Check

- **Posterior predictive checks** is a way to assess the model by comparing the **observed** data to the **predicted** data

::: {.fragment}

```{r}
#| echo: true

pred <- get_predicted(model, data=mtcars, iterations=100) |> 
  as.data.frame() |> 
  reshape_iterations()
  
head(pred)
```

:::

## Posterior Predictive Check - Distribution

- We can see wether the distribution of predicted values are closer to the observed values (in particular, its mean when using linear models)

::: {.fragment}

```{r}
#| echo: true

ggplot(mtcars, aes(x=qsec)) +
  geom_line(data=pred, aes(x=iter_value, group=iter_group), 
            stat="density",  alpha=0.3) +
  geom_density(fill="skyblue", alpha=0.5) +
  theme_minimal() +
  coord_cartesian(ylim=c(0, 1))
```

:::

- Note the use of `geom_line(stat="density")` for the lines instead of `geom_density()` to be able to modify the alpha of the lines (and not the *filling*)
- Posterior predictive checks of the distribution is particularly useful for more complex models to see if the model manages to reproduce the 

## Visualizing the Effects - Datagrid

- We typically are interested in visualizing the **effects** (i.e., the impact of the predictors)
- This means predicting the outcome for different values of the predictor

::: {.fragment}

```{r}
#| echo: true

newdata <- data.frame(mpg = seq(10, 35, length.out=6))
newdata
```

:::

## Visualizing the Effects - Make Predictions


- Predict data on this hypothetical dataset

::: {.fragment}

```{r}
#| echo: true

pred <- get_predicted(model, data=newdata, iterations=250) |> 
  as.data.frame() |> 
  cbind(newdata) 

head(pred)
```

::: 

- `Predicted` column corresponds to the `mean` predicted value
- We want to put the predictions in a "long" format (convenient for plotting)

::: {.fragment}

```{r}
#| echo: true

pred <- reshape_iterations(pred)
head(pred)
```

:::


## Visualizing the Effects - Plot

- Plot predictions with real data points

::: {.fragment}

```{r}
#| echo: true

ggplot(mtcars, aes(x=mpg, y=qsec)) +
  geom_point(size=3) +
  geom_line(data=pred, aes(y=iter_value, group=iter_group), alpha=0.1) +
  geom_line(data=pred, aes(y=Predicted), linewidth=1)
```

:::

- Each line corresponds to one "plausible" regression line (given our posterior beliefs about the model parameters)
- Most of them indicate a positive relationship between `mpg` and `qsec`

## Exercice {background-color="#FFAB91"}

- Is there a relationship between **Petal Length** and **Petal Width** (`iris` builtin dataset)?
  - Use default priors


## The End <sub><sup>(for now)</sup></sub> {.center background-color="#212121"}

*Thank you!*




